/*******************************************************************************
*
* Copyright 2020 SAP SE
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You should have received a copy of the License along with this
* program. If not, you may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*
*******************************************************************************/

package collector

import (
	"fmt"
	"regexp"
	"testing"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"github.com/sapcc/go-bits/assert"
	"github.com/sapcc/go-bits/logg"
	"github.com/sapcc/limes"
	"github.com/sapcc/limes/pkg/db"
	"github.com/sapcc/limes/pkg/test"
)

func Test_RateScrapeSuccess(t *testing.T) {
	rates := []limes.RateInfo{
		{Name: "firstrate"},
		{Name: "secondrate", Unit: limes.UnitKibibytes},
	}
	plugin := test.NewPlugin("unittest", rates...)
	cluster := prepareScrapeTest(t, 2, plugin)
	c := Collector{
		Cluster:  cluster,
		Plugin:   plugin,
		LogError: t.Errorf,
		TimeNow:  test.TimeNow,
		Once:     true,
	}

	//for one of the projects, put some records in for rate limits, to check that
	//the scraper does not mess with those values
	err := db.DB.Insert(&db.ProjectRate{
		ServiceID: 1,
		Name:      "secondrate",
		Limit:     p2u64(10),
		Window:    p2window(1 * limes.WindowSeconds),
	})
	if err != nil {
		t.Fatal(err)
	}
	err = db.DB.Insert(&db.ProjectRate{
		ServiceID: 1,
		Name:      "otherrate",
		Limit:     p2u64(42),
		Window:    p2window(2 * limes.WindowMinutes),
	})
	if err != nil {
		t.Fatal(err)
	}

	//check that ScanDomains created the domain, project and their services; and
	//we set up our initial rates correctly
	test.AssertDBContent(t, "fixtures/ratescrape0.sql")

	//first Scrape should create the entries
	c.ScrapeRates()
	c.ScrapeRates() //twice because there are two projects
	test.AssertDBContent(t, "fixtures/ratescrape1.sql")

	//second Scrape should not change anything (not even the timestamps) since
	//less than 30 minutes have passed since the last Scrape()
	c.ScrapeRates()
	test.AssertDBContent(t, "fixtures/ratescrape1.sql")

	//manually mess with one of the ratesScrapeState
	_, err = db.DB.Exec(`UPDATE project_services SET rates_scrape_state = $1 WHERE id = $2`, `{"firstrate":4096,"secondrate":0}`, 1)
	if err != nil {
		t.Fatal(err)
	}
	//this alone should not cause a new scrape
	c.ScrapeRates()
	test.AssertDBContent(t, "fixtures/ratescrape2.sql")

	//but the changed state will be taken into account when the next scrape is in order
	setProjectServicesRatesStale(t)
	c.ScrapeRates()
	c.ScrapeRates()
	test.AssertDBContent(t, "fixtures/ratescrape3.sql")

	//check data metrics generated by this scraping pass
	registry := prometheus.NewPedanticRegistry()
	amc := &AggregateMetricsCollector{Cluster: cluster}
	registry.MustRegister(amc)
	dmc := &DataMetricsCollector{Cluster: cluster, ReportZeroes: true}
	registry.MustRegister(dmc)
	assert.HTTPRequest{
		Method:       "GET",
		Path:         "/metrics",
		ExpectStatus: 200,
		ExpectBody:   assert.FixtureFile("fixtures/ratescrape_metrics.prom"),
	}.Check(t, promhttp.HandlerFor(registry, promhttp.HandlerOpts{}))

	//check data metrics with the skip_zero flag set
	registry = prometheus.NewPedanticRegistry()
	amc = &AggregateMetricsCollector{Cluster: cluster}
	registry.MustRegister(amc)
	dmc = &DataMetricsCollector{Cluster: cluster, ReportZeroes: false}
	registry.MustRegister(dmc)
	assert.HTTPRequest{
		Method:       "GET",
		Path:         "/metrics",
		ExpectStatus: 200,
		ExpectBody:   assert.FixtureFile("fixtures/ratescrape_metrics_skipzero.prom"),
	}.Check(t, promhttp.HandlerFor(registry, promhttp.HandlerOpts{}))
}

func Test_RateScrapeFailure(t *testing.T) {
	rates := []limes.RateInfo{
		{Name: "firstrate"},
		{Name: "secondrate", Unit: limes.UnitKibibytes},
	}
	plugin := test.NewPlugin("unittest", rates...)
	cluster := prepareScrapeTest(t, 2, plugin)
	c := Collector{
		Cluster:  cluster,
		Plugin:   plugin,
		LogError: t.Errorf,
		TimeNow:  test.TimeNow,
		Once:     true,
	}

	//we will see an expected ERROR during testing, do not make the test fail because of this
	expectedErrorRx := regexp.MustCompile(`^scrape unittest rate data for germany/(berlin|dresden) failed: ScrapeRates failed as requested$`)
	c.LogError = func(msg string, args ...interface{}) {
		msg = fmt.Sprintf(msg, args...)
		if expectedErrorRx.MatchString(msg) {
			logg.Info(msg)
		} else {
			t.Error(msg)
		}
	}

	//check that ScanDomains created the domain, project and their services
	test.AssertDBContent(t, "fixtures/scrape0.sql")

	//ScrapeRates should not touch the DB when scraping fails
	plugin.ScrapeFails = true
	c.ScrapeRates()
	test.AssertDBContent(t, "fixtures/scrape0.sql")
}

func setProjectServicesRatesStale(t *testing.T) {
	t.Helper()
	//make sure that the project is scraped again
	_, err := db.DB.Exec(`UPDATE project_services SET rates_stale = $1`, true)
	if err != nil {
		t.Fatal(err)
	}
}

func p2window(val limes.Window) *limes.Window {
	return &val
}

func Test_ScrapeRatesButNoRates(t *testing.T) {
	plugin := noopQuotaPlugin{}
	cluster := prepareScrapeTest(t, 1, plugin)
	c := Collector{
		Cluster:  cluster,
		Plugin:   plugin,
		LogError: t.Errorf,
		TimeNow:  test.TimeNow,
		Once:     true,
	}

	//check that ScrapeRates() behaves properly when encountering a quota plugin
	//with no Rates() (in the wild, this can happen because some quota plugins
	//only have Resources())
	c.ScrapeRates()
	test.AssertDBContent(t, "fixtures/scrape-no-rates.sql")
}
